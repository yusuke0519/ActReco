{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: CNN with Pytorch using Opp dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('/home/iwasawa/ActReco/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# general\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from progressbar import ProgressBar\n",
    "\n",
    "# dataset\n",
    "from actreco.datasets.opportunityc import Opportunity\n",
    "from actreco.sampling import sampling\n",
    "\n",
    "# modeling\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_batch_generator(x_list, y_list, batch_size, l_sample, nb_iter=1, categorical=False):\n",
    "    nb_sample = sum([x.shape[0] for x in x_list])\n",
    "    idx_set = set(range(nb_sample))\n",
    "    nb_seen_sample = 0\n",
    "    for x in x_list:\n",
    "        nb_seen_sample += x.shape[0]\n",
    "        idx_set -= set(range(nb_seen_sample-l_sample+1, nb_seen_sample+1))\n",
    "    valid_idx = list(idx_set)\n",
    "    \n",
    "    x = np.concatenate(x_list)\n",
    "    y = np.concatenate(y_list)\n",
    "    \n",
    "    for i in range(nb_iter):\n",
    "        start = np.random.choice(valid_idx, batch_size)\n",
    "        X = sampling(x, 'clips', dtype='np', start=start, l_sample=l_sample)[..., np.newaxis].swapaxes(1, 3)\n",
    "        Y = sampling(y, 'clips', dtype='np', start=start, l_sample=l_sample)\n",
    "        Y = np.concatenate([Y.sum(axis=1), np.ones((batch_size, 1))], axis=-1).argmax(axis=-1)\n",
    "        if categorical is not False:\n",
    "            Y = np.eye(categorical)[Y]\n",
    "        yield (X, Y)\n",
    "        \n",
    "        \n",
    "def test_batch_generator(x_list, y_list, batch_size, l_sample, nb_iter=1, categorical=False, seed=0):\n",
    "    \n",
    "    nb_sample = sum([x.shape[0] for x in x_list])\n",
    "    idx_set = set(range(nb_sample))\n",
    "    nb_seen_sample = 0\n",
    "    for x in x_list:\n",
    "        nb_seen_sample += x.shape[0]\n",
    "        idx_set -= set(range(nb_seen_sample-l_sample+1, nb_seen_sample+1))\n",
    "    valid_idx = list(idx_set)\n",
    "    \n",
    "    x = np.concatenate(x_list)\n",
    "    y = np.concatenate(y_list)\n",
    "    \n",
    "    for i in range(nb_iter):\n",
    "        r = np.random.RandomState(seed+i)\n",
    "\n",
    "        start = r.choice(valid_idx, batch_size)\n",
    "        X = sampling(x, 'clips', dtype='np', start=start, l_sample=l_sample)[..., np.newaxis].swapaxes(1, 3)\n",
    "        Y = sampling(y, 'clips', dtype='np', start=start, l_sample=l_sample)\n",
    "        Y = np.concatenate([Y.sum(axis=1), np.ones((batch_size, 1))], axis=-1).argmax(axis=-1)\n",
    "        if categorical is not False:\n",
    "            Y = np.eye(categorical)[Y]\n",
    "        yield (X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set = Opportunity(userID='S1,S2,S3')\n",
    "x_train_list = [x[1] for x in train_set.data_list()]\n",
    "y_train_list = [x[2] for x in train_set.data_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_set = Opportunity(userID='S4')\n",
    "x_test_list = [x[1] for x in test_set.data_list()]\n",
    "y_test_list = [x[2] for x in test_set.data_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VanilaCNN(nn.Module):\n",
    "    def __init__(self, l_sample, nb_modal, nb_in_filter, nb_filter_list, kw, nb_unit, nb_out):\n",
    "        super(VanilaCNN, self).__init__()\n",
    "        self.l_sample = l_sample\n",
    "        self.nb_modal = nb_modal\n",
    "        self.nb_conv = len(nb_filter_list)\n",
    "        \n",
    "        if isinstance(kw, int):\n",
    "            kw = [kw] * self.nb_conv\n",
    "        assert len(nb_filter_list) == len(kw)\n",
    "        nb_filter_list = [nb_in_filter] + nb_filter_list\n",
    "        \n",
    "        self.kw = kw\n",
    "        self.nb_filter_list = nb_filter_list\n",
    "\n",
    "        convs = []\n",
    "        for i in range(self.nb_conv):\n",
    "            convs.append(nn.Conv2d(nb_filter_list[i], nb_filter_list[i+1], (1, kw[i])))\n",
    "        self.convs = nn.ModuleList(convs)\n",
    "        self.fc1 = nn.Linear(self.conv_output_shape(), nb_unit)\n",
    "        self.fc2 = nn.Linear(nb_unit, nb_out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for conv in self.convs:\n",
    "            x = F.max_pool2d(F.relu(conv(x)), (1, 2))\n",
    "            dropout = nn.Dropout2d(0.5)\n",
    "            # x = dropout(x)\n",
    "        x = x.view(-1, self.conv_output_shape())\n",
    "        x = F.dropout(self.fc1(x), 0.5)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def conv_output_shape(self):\n",
    "        length = self.l_sample\n",
    "        for i in range(self.nb_conv):\n",
    "            length = int((length - self.kw[i] + 1) /2)\n",
    "        return length * self.nb_filter_list[-1] * self.nb_modal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_modal_dim = 2\n",
    "nb_modal = train_set.nb_modal\n",
    "nb_out = train_set.nb_class\n",
    "\n",
    "kw = 3\n",
    "nb_filter_list = [50, 40, 20]\n",
    "nb_in_filter = 1\n",
    "drop_rate = 0.5\n",
    "nb_unit = 400\n",
    "\n",
    "batch_size = 128\n",
    "l_sample = 30\n",
    "interval = int(l_sample) * 0.5\n",
    "\n",
    "nb_iter = 100\n",
    "report_each = 10\n",
    "\n",
    "model = VanilaCNN(l_sample=l_sample, nb_modal=nb_modal, nb_in_filter=1, nb_filter_list=nb_filter_list, kw=kw, nb_unit=nb_unit, nb_out=nb_out).cuda()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13% (13 of 100) |###                      | Elapsed Time: 0:00:00 ETA: 0:00:05"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280 samples, 10 batch \t Loss: 2.501356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22% (22 of 100) |#####                    | Elapsed Time: 0:00:01 ETA: 0:00:04"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2560 samples, 20 batch \t Loss: 1.417248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33% (33 of 100) |########                 | Elapsed Time: 0:00:01 ETA: 0:00:03"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3840 samples, 30 batch \t Loss: 1.461235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44% (44 of 100) |###########              | Elapsed Time: 0:00:02 ETA: 0:00:02"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5120 samples, 40 batch \t Loss: 1.640611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54% (54 of 100) |#############            | Elapsed Time: 0:00:02 ETA: 0:00:02"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6400 samples, 50 batch \t Loss: 1.369733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64% (64 of 100) |################         | Elapsed Time: 0:00:03 ETA: 0:00:01"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7680 samples, 60 batch \t Loss: 1.437686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73% (73 of 100) |##################       | Elapsed Time: 0:00:03 ETA: 0:00:01"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8960 samples, 70 batch \t Loss: 1.601655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84% (84 of 100) |#####################    | Elapsed Time: 0:00:04 ETA: 0:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10240 samples, 80 batch \t Loss: 1.534630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94% (94 of 100) |#######################  | Elapsed Time: 0:00:04 ETA: 0:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11520 samples, 90 batch \t Loss: 1.517131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% (99 of 100) |######################## | Elapsed Time: 0:00:05 ETA: 0:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800 samples, 100 batch \t Loss: 1.462462\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "p = ProgressBar(max_value=nb_iter)\n",
    "gen = train_batch_generator(x_train_list, y_train_list, batch_size, l_sample, nb_iter=nb_iter, categorical=False)\n",
    "for batch_idx, (X, y) in enumerate(gen):\n",
    "    batch_idx += 1\n",
    "    tX = Variable(torch.from_numpy(X.astype('float32')), requires_grad=False).cuda()\n",
    "    ty = Variable(torch.LongTensor(y), requires_grad=False).cuda()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    output = model(tX)\n",
    "    \n",
    "    loss = criterion(output, ty)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if batch_idx % report_each == 0:\n",
    "        print(\"{} samples, {} batch \\t Loss: {:.6f}\".format(batch_idx * batch_size, batch_idx, loss.data[0]))\n",
    "    p.update(batch_idx)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def test(model, x_list, y_list, l_sample, nb_iter=1, batch_size=2048):\n",
    "    model.eval()\n",
    "    test_gen = test_batch_generator(x_list, y_list, batch_size, l_sample, nb_iter=nb_iter, categorical=False)\n",
    "    y_list = []\n",
    "    py_list = []\n",
    "    for X, y in test_gen:\n",
    "        tX = Variable(torch.from_numpy(X.astype('float32')), requires_grad=False).cuda()\n",
    "        py = model(tX).max(1)[1].data.cpu().numpy().reshape(-1)\n",
    "        y_list.append(y)\n",
    "        py_list.append(py)\n",
    "    \n",
    "    y = np.concatenate(y_list)\n",
    "    py = np.concatenate(py_list)\n",
    "    print(metrics.accuracy_score(y, py))\n",
    "    print(metrics.f1_score(y, py, average='macro'))\n",
    "    print(metrics.precision_score(y, py, average=None))\n",
    "    print(metrics.recall_score(y, py, average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    }
   ],
   "source": [
    "%prun test_batch_generator(x_test_list, y_test_list, batch_size, l_sample, nb_iter=nb_iter, categorical=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6171875\n",
      "0.0448991190679\n",
      "[ 0.         0.         0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.         0.         0.\n",
      "  0.6171875]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iwasawa/.pyenv/versions/anaconda3-2.5.0/envs/env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/iwasawa/.pyenv/versions/anaconda3-2.5.0/envs/env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "%prun test(model, x_test_list, y_test_list, l_sample, nb_iter=1, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    }
   ],
   "source": [
    "%prun np.concatenate(x_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keras model for comparison\n",
    "# which is much slower than pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: GeForce GTX TITAN X (CNMeM is disabled, cuDNN Mixed dnn version. The header is from one version, but we link with a different version (4007, 6021))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 50, 113, 28)       200       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 50, 113, 28)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 50, 113, 14)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 40, 113, 12)       6040      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 40, 113, 12)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 40, 113, 6)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 20, 113, 4)        2420      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 20, 113, 4)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 20, 113, 2)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4520)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 400)               1808400   \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 18)                7218      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 18)                0         \n",
      "=================================================================\n",
      "Total params: 1,824,278\n",
      "Trainable params: 1,824,278\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Convolution2D, MaxPool2D, Dropout, Activation, Flatten\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(50, kernel_size=(1, 3), input_shape=[1, nb_modal, l_sample]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool2D(pool_size=(1, 2)))\n",
    "model.add(Convolution2D(40, (1, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool2D(pool_size=(1, 2)))\n",
    "model.add(Convolution2D(20, (1, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool2D(pool_size=(1, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(nb_unit))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(nb_out))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "optimizer = SGD(lr=0.01, momentum=0.5)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): install mkl with `conda install mkl-service`: No module named 'mkl'\n",
      " 10% (1 of 10) |#             | Elapsed Time: 0:00:01 ETA: 159109 days, 8:59:59"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seen 1280 samples, 10 batch \t Loss: 2.066165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10% (1 of 10) |#             | Elapsed Time: 0:00:03 ETA: 166697 days, 4:29:59"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seen 2560 samples, 20 batch \t Loss: 1.473514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10% (1 of 10) |#            | Elapsed Time: 0:00:05 ETA: 164016 days, 23:29:59"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seen 3840 samples, 30 batch \t Loss: 1.507653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (10 of 10) |#########################| Elapsed Time: 0:00:06 ETA:  0:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seen 5120 samples, 40 batch \t Loss: 1.679058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10% (1 of 10) |#             | Elapsed Time: 0:00:08 ETA: 173419 days, 1:29:59"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seen 6400 samples, 50 batch \t Loss: 1.486398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10% (1 of 10) |#             | Elapsed Time: 0:00:10 ETA: 159364 days, 1:30:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seen 7680 samples, 60 batch \t Loss: 1.435246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (10 of 10) |#########################| Elapsed Time: 0:00:12 ETA:  0:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seen 8960 samples, 70 batch \t Loss: 1.382977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10% (1 of 10) |#            | Elapsed Time: 0:00:14 ETA: 169608 days, 12:59:59"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seen 10240 samples, 80 batch \t Loss: 1.229994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10% (1 of 10) |#             | Elapsed Time: 0:00:15 ETA: 157792 days, 1:59:59"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seen 11520 samples, 90 batch \t Loss: 1.508772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (10 of 10) |#########################| Elapsed Time: 0:00:17 ETA:  0:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seen 12800 samples, 100 batch \t Loss: 1.485959\n"
     ]
    }
   ],
   "source": [
    "p = ProgressBar(max_value=report_each)\n",
    "gen = train_batch_generator(x_train_list, y_train_list, batch_size, l_sample, nb_iter=nb_iter, categorical=train_set.nb_class)\n",
    "for batch_idx, (X, y) in enumerate(gen):\n",
    "    batch_idx += 1\n",
    "    loss = model.train_on_batch(X, y)\n",
    "    if batch_idx % report_each == 0:\n",
    "        print(\"Seen {} samples, {} batch \\t Loss: {:.6f}\".format(batch_idx * batch_size, batch_idx, float(loss)))\n",
    "        p.update(report_each)\n",
    "    else:\n",
    "        p.update((batch_idx % report_each))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
